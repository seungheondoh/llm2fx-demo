<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LLM2Fx: Text to Audio Effects Parameters</title>
    <link rel="stylesheet" href="style.css">
    <style>
        .original-players {
            justify-content: flex-start;
        }

        .original-player {
            text-align: left;
        }

        .model-player {
            cursor: pointer;
            padding: 10px;
            border: 2px solid transparent;
            border-radius: 8px;
            transition: border-color 0.3s ease;
        }

        .model-player:hover {
            border-color: #ddd;
        }

        .model-player.selected {
            border-color: #007bff;
            background-color: #f8f9fa;
        }

        .param-container {
            margin-top: 15px;
        }

        .param-json {
            background-color: #f8f9fa;
            border: 1px solid #e9ecef;
            border-radius: 4px;
            padding: 15px;
            font-family: 'Courier New', monospace;
            font-size: 14px;
            line-height: 1.4;
            overflow-x: auto;
            white-space: pre-wrap;
            color: #333;
        }

        .prompt-container {
            margin-top: 15px;
            background-color: #f8f9fa;
            border: 1px solid #e9ecef;
            border-radius: 8px;
            overflow: hidden;
        }

        .prompt-code {
            margin: 0;
            padding: 20px;
            background-color: #2d3748;
            color: #e2e8f0;
            font-family: 'Fira Code', 'Consolas', 'Monaco', 'Courier New', monospace;
            font-size: 13px;
            line-height: 1.5;
            overflow-x: auto;
            white-space: pre-wrap;
        }

        .prompt-code code {
            background: none;
            padding: 0;
            color: inherit;
            font-size: inherit;
            font-family: inherit;
        }

        .python {
            /* Python syntax highlighting could be added here */
        }
    </style>
</head>

<body>
    <header>
        <h1>LLM2Fx: Can Large Language Models Predict Audio Effects Parameters from Natural Language?</h1>
        <div class="authors">
            <p>A framework leveraging Large Language Models (LLMs) to predict Fx parameters directly from text</p>
        </div>
        <div class="links">
            <a href="#" target="_blank" class="paper-link" style="margin: 0 5px">ðŸ“„ Paper</a>
            <a href="https://github.com/SonyResearch/LLM2Fx" target="_blank" class="code-link" style="margin: 0 5px">ðŸ’»
                Code</a>
            <a href="https://huggingface.co/collections/seungheondoh/llm2fx-6821b961b982fe1eab1b00bf" target="_blank"
                class="dataset-link" style="margin: 0 5px">ðŸ¤— Dataset</a>
        </div>
        <img src="static/img/front.png" alt="LLM2Fx Framework Overview"
            style="width: 100%; margin: 20px 0; display: block; margin-left: auto; margin-right: auto;">

    </header>

    <section class="abstract">
        <h2>Abstract</h2>
        <p>
            In music production, manipulating audio effects (Fx) parameters through natural language has the potential
            to reduce technical barriers for non-experts.
            We present <strong>LLM2Fx</strong>, a framework leveraging Large Language Models (LLMs) to predict Fx
            parameters directly from textual descriptions
            without requiring task-specific training or fine-tuning. Our approach addresses the text-to-effect parameter
            prediction (Text2Fx) task
            by mapping natural language descriptions to the corresponding Fx parameters for equalization and
            reverberation.
            We demonstrate that LLMs can generate Fx parameters in a zero-shot manner that elucidates the relationship
            between
            timbre semantics and audio effects in music production.
        </p>
    </section>

    <section class="demo-controls">
        <h2>Interactive Demo</h2>
        <p>Compare audio effects generated by different models based on natural language descriptions.</p>

        <div class="control-panel">
            <div class="selector">
                <label for="instrument">Instrument:</label>
                <select id="instrument">
                    <option value="drums">Drums</option>
                    <option value="guitar">Guitar</option>
                    <option value="piano">Piano</option>
                </select>
            </div>

            <div class="selector">
                <label for="effect">Effect Type:</label>
                <select id="effect">
                    <option value="reverb">Reverb</option>
                    <option value="eq">EQ</option>
                </select>
            </div>

            <div class="selector">
                <label for="variant">Description:</label>
                <select id="variant">
                    <!-- Will be populated dynamically based on effect type -->
                </select>
            </div>
        </div>
    </section>

    <section class="original-audio">
        <h2>Original Audio</h2>
        <div class="original-players">
            <div class="original-player" data-instrument="guitar">
                <h3>Guitar</h3>
                <audio controls>
                    <source src="static/audio/original/guitar.mp3" type="audio/mpeg">
                    Your browser does not support the audio element.
                </audio>
            </div>
            <div class="original-player" data-instrument="piano">
                <h3>Piano</h3>
                <audio controls>
                    <source src="static/audio/original/piano.mp3" type="audio/mpeg">
                    Your browser does not support the audio element.
                </audio>
            </div>
            <div class="original-player" data-instrument="drums">
                <h3>Drums</h3>
                <audio controls>
                    <source src="static/audio/original/drums.mp3" type="audio/mpeg">
                    Your browser does not support the audio element.
                </audio>
            </div>
        </div>
    </section>

    <section class="audio-comparison">
        <h2>Predicted Generated Audio</h2>
        <p>
            The audio effects generated by the models are compared to the original audio.
            If you click on the model, the parameters will be displayed.
        </p>
        <div class="models-container">
            <!-- Audio players will be added here dynamically -->
        </div>

        <div class="parameter-visualization">
            <div id="parameter-display">
                <p> </p>
            </div>
        </div>
    </section>

    <section class="prompt-design">
        <h2>LLM Prompt for inference</h2>
        <small>
            <p>
                To effectively leverage LLMs for Fx parameter generation, we design a specialized system prompt that
                frames the task and provides necessary constraints. Our system prompt serves three key functions:
                <br /> <b>1 Role Definition</b>: establishes the LLM as an expert audio engineer with specialized
                knowledge
                in sound design and audio processing; <br /> <b>2 Task Instruction</b>: clearly defines the objective of
                translating natural language description, such as semantic word, instrument type, Fx type, into specific
                audio effect parameters; <br /> <b>3 Response Format</b>: enforces structured output in JSON format to
                ensure parameter predictions are machine-readable. This prompt design ensures that the LLM generates
                structured parameter predictions while leveraging its implicit knowledge of audio processing concepts.
            </p>
        </small>
        <div class="prompt-container">
            <pre class="prompt-code"><code class="python">"""
You are an expert audio engineer and music producer specializing in sound design and audio processing.
Your task is to translate descriptive timbre words into specific audio effect parameters that will
achieve the desired sound character. You have deep knowledge of equalizers and understand how they shape
timbre. You MUST respond with ONLY a valid JSON object. Do not include any explanatory text, markdown
formatting, or code blocks. Your entire response should parse as valid JSON.

# Instruction Format
Given a reverb description word or phrase and an instrument type, generate appropriate parameters for a
frequency-dependent reverb that will achieve the requested spatial character.
For 44100 sample rate audio, Consider the typical reverb needs of the specified instrument when
designing the reverb characteristics.

# Input Format
The input will consist of:
1. A reverb description such as:
   - Single words: "hall", "room", "plate", "cathedral", "chamber", "spring", "ambient"
   - Combined descriptions: "warm hall", "bright room", "dark chamber", "short but dense"
   - Spatial descriptions: "distant", "close", "intimate", "huge", "airy", "tight"
2. An instrument type such as:
   - "drums", "guitar", "piano", "vocals", "strings", "brass"

# Output Format
Respond with a JSON object containing precise numerical parameters for the reverb. All values should be
in float format for efficiency. The output will include:
1. The reverb parameters optimized for the requested spatial character and instrument. All values should
   be floating point numbers with 2 decimal places of precision.
2. A detailed explanation of how the chosen parameters achieve the desired reverb effect

Format:
{
    "reverb": {
        "band0_gain": float,
        "band1_gain": float,
        "band2_gain": float,
        "band3_gain": float,
        "band4_gain": float,
        "band5_gain": float,
        "band6_gain": float,
        "band7_gain": float,
        "band8_gain": float,
        "band9_gain": float,
        "band10_gain": float,
        "band11_gain": float,
        "band0_decay": float,
        "band1_decay": float,
        "band2_decay": float,
        "band3_decay": float,
        "band4_decay": float,
        "band5_decay": float,
        "band6_decay": float,
        "band7_decay": float,
        "band8_decay": float,
        "band9_decay": float,
        "band10_decay": float,
        "band11_decay": float,
        "mix": float
    },
    "reason": str
}
"""</code></pre>
        </div>
    </section>

    <footer>
        <p>Â© 2025 LLM2Fx Research Team</p>
    </footer>

    <script>
        // Load the master data file
        const masterDataUrl = 'static/master.json';
        let masterData;

        // DOM elements
        const instrumentSelect = document.getElementById('instrument');
        const effectSelect = document.getElementById('effect');
        const variantSelect = document.getElementById('variant');
        const modelsContainer = document.querySelector('.models-container');
        const parameterDisplay = document.getElementById('parameter-display');

        // Fetch the master data
        fetch(masterDataUrl)
            .then(response => response.json())
            .then(data => {
                masterData = data;
                initializeDemo();
            })
            .catch(error => console.error('Error loading master data:', error));
        // Initialize the demo interface
        function initializeDemo() {
            // Set up event listeners
            instrumentSelect.addEventListener('change', updateEffectOptions);
            effectSelect.addEventListener('change', updateVariantOptions);
            variantSelect.addEventListener('change', updateAudioPlayers);

            // Show only the initially selected instrument
            const initialInstrument = instrumentSelect.value;
            document.querySelectorAll('.original-player').forEach(player => {
                player.style.display = player.getAttribute('data-instrument') === initialInstrument ? 'block' : 'none';
            });

            // Initialize the interface
            updateEffectOptions();
        }
        // Update effect options based on selected instrument
        function updateEffectOptions() {
            const selectedInstrument = instrumentSelect.value;
            const instrumentData = masterData.instruments.find(i => i.name === selectedInstrument);

            // Show only the selected instrument's original player
            document.querySelectorAll('.original-player').forEach(player => {
                player.style.display = player.getAttribute('data-instrument') === selectedInstrument ? 'block' : 'none';
            });

            // Update effect options
            if (instrumentData && instrumentData.effects) {
                updateVariantOptions();
            }
        }

        // Update variant options based on selected effect
        function updateVariantOptions() {
            const selectedInstrument = instrumentSelect.value;
            const selectedEffect = effectSelect.value;

            const instrumentData = masterData.instruments.find(i => i.name === selectedInstrument);
            const effectData = instrumentData.effects.find(e => e.fx_type === selectedEffect);

            // Clear existing options
            variantSelect.innerHTML = '';

            // Add new options
            if (effectData && effectData.variants) {
                effectData.variants.forEach(variant => {
                    const option = document.createElement('option');
                    option.value = variant.name;
                    option.textContent = variant.name;
                    variantSelect.appendChild(option);
                });

                // Update audio players
                updateAudioPlayers();
            }
        }

        // Update audio players based on selected options
        function updateAudioPlayers() {
            const selectedInstrument = instrumentSelect.value;
            const selectedEffect = effectSelect.value;
            const selectedVariant = variantSelect.value;

            // Find the relevant data
            const instrumentData = masterData.instruments.find(i => i.name === selectedInstrument);
            const effectData = instrumentData.effects.find(e => e.fx_type === selectedEffect);
            const variantData = effectData.variants.find(v => v.name === selectedVariant);

            // Clear existing players
            modelsContainer.innerHTML = '';

            // Create new players
            if (variantData && variantData.models) {
                variantData.models.forEach((model, index) => {
                    const modelDiv = document.createElement('div');
                    modelDiv.className = 'model-player';
                    if (index === 0) modelDiv.classList.add('selected'); // Auto-select first model

                    const modelName = document.createElement('h3');
                    modelName.textContent = formatModelName(model.name);
                    modelDiv.appendChild(modelName);

                    const audioPlayer = document.createElement('audio');
                    audioPlayer.controls = true;
                    audioPlayer.src = model.audio_path;
                    modelDiv.appendChild(audioPlayer);

                    // Add click handler to display parameters
                    modelDiv.addEventListener('click', () => {
                        // Remove selected class from all models
                        document.querySelectorAll('.model-player').forEach(player => {
                            player.classList.remove('selected');
                        });
                        // Add selected class to clicked model
                        modelDiv.classList.add('selected');
                        displayParameters(model);
                    });

                    modelsContainer.appendChild(modelDiv);
                });

                // Auto-display parameters for the first model
                if (variantData.models.length > 0) {
                    displayParameters(variantData.models[0]);
                }
            } else {
                // Reset parameter display if no models
                parameterDisplay.innerHTML = '<p>No models available</p>';
            }
        }

        // Display parameters for selected model
        function displayParameters(model) {
            // Clear existing content
            parameterDisplay.innerHTML = '';

            // Add model name
            const modelTitle = document.createElement('h4');
            modelTitle.textContent = formatModelName(model.name) + ' Parameters';
            parameterDisplay.appendChild(modelTitle);

            // If there's a JSON path, fetch and display parameters
            if (model.json_path && model.json_path !== '') {
                fetch(model.json_path)
                    .then(response => response.json())
                    .then(params => {
                        // Create parameter display as formatted JSON string
                        const paramContainer = document.createElement('div');
                        paramContainer.className = 'param-container';

                        const jsonString = JSON.stringify(params, null, 2);
                        const preElement = document.createElement('pre');
                        preElement.className = 'param-json';
                        preElement.textContent = jsonString;

                        paramContainer.appendChild(preElement);
                        parameterDisplay.appendChild(paramContainer);
                    })
                    .catch(error => {
                        parameterDisplay.innerHTML += '<p>Error loading parameters</p>';
                        console.error('Error loading parameters:', error);
                    });
            } else if (model.name === 'ground_truth') {
                parameterDisplay.innerHTML += '<p>Ground truth reference audio</p>';
            } else {
                parameterDisplay.innerHTML += '<p>No parameters available</p>';
            }
        }

        // Format model name for display
        function formatModelName(name) {
            if (name === 'ground_truth') return 'Ground Truth';
            if (name === 'llm2fx') return 'LLM2Fx (Ours)';
            if (name === 'gpt4o') return 'GPT-4o';
            if (name === 'chu2025text2fx') return 'Chu2025text2fx';
            return name.charAt(0).toUpperCase() + name.slice(1);
        }

        // Format parameter name for display
        function formatParamName(name) {
            return name
                .replace(/_/g, ' ')
                .replace(/\b\w/g, l => l.toUpperCase());
        }

        // Format parameter value for display
        function formatParamValue(value) {
            if (typeof value === 'number') {
                return value.toFixed(2);
            }
            return value;
        }
    </script>
</body>

</html>
