<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LLM2Fx: Text to Audio Effects Parameters</title>
    <link rel="stylesheet" href="style.css">
    <style>
        .original-players {
            justify-content: flex-start;
        }

        .original-player {
            text-align: left;
        }

        .model-player {
            cursor: pointer;
            padding: 10px;
            border: 2px solid transparent;
            border-radius: 8px;
            transition: border-color 0.3s ease;
        }

        .model-player:hover {
            border-color: #ddd;
        }

        .model-player.selected {
            border-color: #007bff;
            background-color: #f8f9fa;
        }

        .param-container {
            margin-top: 15px;
        }

        .param-json {
            background-color: #f8f9fa;
            border: 1px solid #e9ecef;
            border-radius: 4px;
            padding: 15px;
            font-family: 'Courier New', monospace;
            font-size: 14px;
            line-height: 1.4;
            overflow-x: auto;
            white-space: pre-wrap;
            color: #333;
        }

        .prompt-container {
            margin-top: 15px;
            background-color: #f8f9fa;
            border: 1px solid #e9ecef;
            border-radius: 8px;
            overflow: hidden;
        }

        .prompt-code {
            margin: 0;
            padding: 20px;
            background-color: #2d3748;
            color: #e2e8f0;
            font-family: 'Fira Code', 'Consolas', 'Monaco', 'Courier New', monospace;
            font-size: 13px;
            line-height: 1.5;
            overflow-x: auto;
            white-space: pre-wrap;
        }

        .prompt-code code {
            background: none;
            padding: 0;
            color: inherit;
            font-size: inherit;
            font-family: inherit;
        }

        .python {
            /* Python syntax highlighting could be added here */
        }
    </style>
</head>

<body>
    <header>
        <h1>LLM2Fx: Can Large Language Models Predict Audio Effects Parameters from Natural Language?</h1>
        <div class="authors">
            <p>A framework leveraging Large Language Models (LLMs) to predict Fx parameters directly from text</p>
        </div>
        <div class="links">
            <a href="https://arxiv.org/abs/2505.20770v1" target="_blank" class="paper-link" style="margin: 0 5px">ðŸ“„
                Paper</a>
            <a href="https://github.com/SonyResearch/LLM2Fx" target="_blank" class="code-link" style="margin: 0 5px">ðŸ’»
                Code</a>
            <a href="https://huggingface.co/collections/seungheondoh/llm2fx-6821b961b982fe1eab1b00bf" target="_blank"
                class="dataset-link" style="margin: 0 5px">ðŸ¤— Dataset</a>
        </div>
        <img src="static/img/front.png" alt="LLM2Fx Framework Overview"
            style="width: 100%; margin: 20px 0; display: block; margin-left: auto; margin-right: auto;">

    </header>

    <section class="abstract">
        <h2>Abstract</h2>
        <p>
            In music production, manipulating audio effects~(Fx) parameters through natural language has the potential
            to reduce technical barriers for non-experts. We present LLM2Fx, a framework leveraging Large Language
            Models (LLMs) to predict Fx parameters directly from textual descriptions without requiring task-specific
            training or fine-tuning. Our approach address the text-to-effect parameter prediction (Text2Fx) task by
            mapping natural language descriptions to the corresponding Fx parameters for equalization and reverberation.
            We demonstrate that LLMs can generate Fx parameters in a zero-shot manner that elucidates the relationship
            between timbre semantics and audio effects in music production. To enhance performance, we introduce three
            types of in-context examples: audio Digital Signal Processing (DSP) features, DSP function code, and
            few-shot examples. Our results demonstrate that LLM-based Fx parameter generation outperforms previous
            optimization approaches, offering competitive performance in translating natural language descriptions to
            appropriate Fx settings. Furthermore, LLMs can serve as text-driven interfaces for audio production, paving
            the way for more intuitive and accessible music production tools.
        </p>
    </section>

    <section class="demo-controls">
        <h2>Interactive Demo</h2>
        <p>Compare audio effects generated by different models based on natural language descriptions.</p>

        <div class="control-panel">
            <div class="selector">
                <label for="instrument">Instrument:</label>
                <select id="instrument">
                    <option value="drums">Drums</option>
                    <option value="guitar">Guitar</option>
                    <option value="piano">Piano</option>
                </select>
            </div>

            <div class="selector">
                <label for="effect">Effect Type:</label>
                <select id="effect">
                    <option value="reverb">Reverb</option>
                    <option value="eq">EQ</option>
                </select>
            </div>

            <div class="selector">
                <label for="variant">Description:</label>
                <select id="variant">
                    <!-- Will be populated dynamically based on effect type -->
                </select>
            </div>
        </div>
    </section>

    <section class="original-audio">
        <h2>Original Audio</h2>
        <div class="original-players">
            <div class="original-player" data-instrument="guitar">
                <h3>Guitar</h3>
                <audio controls>
                    <source src="static/audio/original/guitar.mp3" type="audio/mpeg">
                    Your browser does not support the audio element.
                </audio>
            </div>
            <div class="original-player" data-instrument="piano">
                <h3>Piano</h3>
                <audio controls>
                    <source src="static/audio/original/piano.mp3" type="audio/mpeg">
                    Your browser does not support the audio element.
                </audio>
            </div>
            <div class="original-player" data-instrument="drums">
                <h3>Drums</h3>
                <audio controls>
                    <source src="static/audio/original/drums.mp3" type="audio/mpeg">
                    Your browser does not support the audio element.
                </audio>
            </div>
        </div>
    </section>

    <section class="audio-comparison">
        <h2>Predicted Generated Audio</h2>
        <p>
            The audio effects generated by the models are compared to the original audio.
            If you click on the model, the parameters will be displayed.
        </p>
        <div class="models-container">
            <!-- Audio players will be added here dynamically -->
        </div>

        <div class="parameter-visualization">
            <div id="parameter-display">
                <p> </p>
            </div>
        </div>
    </section>

    <section class="prompt-design">
        <h2>LLM Prompt for inference</h2>
        <p>
            To effectively leverage LLMs for Fx parameter generation, we design a specialized system prompt that
            frames the task and provides necessary constraints. Our system prompt serves four key functions:
            <br /> <b>1 Role Definition</b>: establishes the LLM as an expert audio engineer with specialized
            knowledge
            in sound design and audio processing; <br /> <b>2 Task Instruction</b>: clearly defines the objective of
            translating natural language description, such as semantic word, instrument type, Fx type, into specific
            audio effect parameters; <br /> <b>3 Response Format</b>: enforces structured output in JSON format to
            ensure parameter predictions are machine-readable. This prompt design ensures that the LLM generates
            structured parameter predictions while leveraging its implicit knowledge of audio processing concepts.
            <br /> <b>4. Incontext Information</b>: provides three types of in-context information to enhance
            performance: (1) DSP function code that implements the audio effects, (2) DSP features that capture
            audio characteristics, and (3) few-shot examples showing input-output pairs for incontext learning.
        </p>
        <div class="prompt-container">
            <pre class="prompt-code"><code class="python">"""
You are an expert audio engineer and music producer specializing in sound design and audio processing. Your task is to translate descriptive timbre words into specific audio effects parameters that will achieve the desired sound character. You have deep knowledge of equalizers and understand how they shape timbre. You MUST respond with ONLY a valid JSON object.

# Instruction Format
Given a reverb description word or phrase and an instrument type, generate appropriate parameters for a frequency-dependent reverb that will achieve the requested spatial character.
For 44100 sample rate audio, Consider the typical reverb needs of the specified instrument when designing the reverb characteristics.

# Input Format
The input will consist of:
1. A reverb description such as:
- Single words: "hall", "room", "plate", "cathedral", "chamber", "spring", "ambient"
- Combined descriptions: "warm hall", "bright room", "dark chamber", "short but dense"
- Spatial descriptions: "distant", "close", "intimate", "huge", "airy", "tight"
2. An instrument type such as:
- "drums", "guitar", "piano", "vocals", "strings", "brass"

# Output Format
Respond with a JSON object containing precise numerical parameters for the reverb. All values should be in float format for efficiency. The output will include:
1. The reverb parameters optimized for the requested spatial character and instrument. All values should be floating point numbers with 2 decimal places of precision.
2. A detailed explanation of how the chosen parameters achieve the desired reverb effect
Format:
{
"reverb": {
"band0_gain": float,
"band1_gain": float,
"band2_gain": float,
"band3_gain": float,
"band4_gain": float,
...
[THE REST OF THE PARAMETERS ARE OMITTED]
},
"reason": str
}

# Signal processing function

import numpy as np
import math
import scipy.signal
from scipy.fft import rfft, irfft
from functools import partial

def noise_shaped_reverberation(
x,  # Shape: (channels, seq_len)
sample_rate,
band0_gain,
...
band11_decay,
mix
):
[THE REST OF THE PYTHON CODE ARE OMITTED]

# Input audio feature

{'sample_rate': 44100, 'rms_energy': 0.126, 'crest_factor': 6.091, 'dynamic_spread': 0.312, 'spectral_centroid': 461.47, 'spectral_flatness': 0.001, 'spectral_bandwidth': 718.456}

# Incontext examples

QUESTION: please design a reverb audio effects for a echo piano sound.
ANSWER: {'reverb': {'band0_gain': 0.0, 'band1_gain': 0.0, 'band2_gain': 0.0, 'band3_gain': 0.0, 'band4_gain': 0.0, 'band5_gain': 0.0, 'band6_gain': 0.0, 'band7_gain': 0.0, 'band8_gain': 0.0, 'band9_gain': 0.0, 'band10_gain': 0.0, 'band11_gain': 0.0, 'band0_decay': 0.1, 'band1_decay': 0.1, 'band2_decay': 0.1, 'band3_decay': 0.1, 'band4_decay': 0.1, 'band5_decay': 0.1, 'band6_decay': 0.1, 'band7_decay': 0.1, 'band8_decay': 0.1, 'band9_decay': 0.1, 'band10_decay': 0.1, 'band11_decay': 0.1, 'mix': 0.8}, 'reason': 'Creating an echo effect for piano, using a short decay time to simulate quick reflections and a modest mix level to maintain clarity and space.'}

QUESTION: please design a reverb audio effects for a warm piano sound.
ANSWER: {'reverb': {'band0_gain': 0.05, 'band1_gain': 0.1, 'band2_gain': 0.15, 'band3_gain': 0.2, 'band4_gain': 0.25, 'band5_gain': 0.3, 'band6_gain': 0.35, 'band7_gain': 0.4, 'band8_gain': 0.45, 'band9_gain': 0.5, 'band10_gain': 0.55, 'band11_gain': 0.6, 'band0_decay': 1.2, 'band1_decay': 1.8, 'band2_decay': 2.5, 'band3_decay': 3.5, 'band4_decay': 4.5, 'band5_decay': 6.0, 'band6_decay': 7.5, 'band7_decay': 9.5, 'band8_decay': 11.5, 'band9_decay': 14.0, 'band10_decay': 16.5, 'band11_decay': 19.5, 'mix': 0.8}, 'reason': 'Designed a warm sound for piano by applying a boost to the lower and middle frequency bands, creating a rich and full-bodied character. The decay times gradually increase to emphasize the warmth and produce a sense of intimacy. The mix level is set to achieve a balanced blend between the dry signal and the reverb.'}

QUESTION: please design a reverb audio effects for a distorted guitar sound.
ANSWER: {'reverb': {'band0_gain': 0.05, 'band1_gain': 0.1, 'band2_gain': 0.15, 'band3_gain': 0.2, 'band4_gain': 0.25, 'band5_gain': 0.2, 'band6_gain': 0.15, 'band7_gain': 0.1, 'band8_gain': 0.05, 'band9_gain': 0.02, 'band10_gain': 0.01, 'band11_gain': 0.0, 'band0_decay': 1.0, 'band1_decay': 0.8, 'band2_decay': 0.6, 'band3_decay': 0.4, 'band4_decay': 0.2, 'band5_decay': 0.1, 'band6_decay': 0.05, 'band7_decay': 0.02, 'band8_decay': 0.01, 'band9_decay': 0.005, 'band10_decay': 0.002, 'band11_decay': 0.001, 'mix': 0.8}, 'reason': 'The reverb is designed to enhance the distorted guitar sound by boosting the low and mid frequencies while gradually reducing the highs. The shorter decay times contribute to a tight and focused reverb tail. The lower mix value ensures the dry signal dominates while still adding a subtle sense of space.'}

QUESTION: please design a reverb audio effects for a echo guitar sound.
ANSWER: {'reverb': {'band0_gain': 0.0, 'band1_gain': 0.1, 'band2_gain': 0.2, 'band3_gain': 0.3, 'band4_gain': 0.3, 'band5_gain': 0.2, 'band6_gain': 0.1, 'band7_gain': 0.05, 'band8_gain': 0.02, 'band9_gain': 0.01, 'band10_gain': 0.01, 'band11_gain': 0.01, 'band0_decay': 0.1, 'band1_decay': 0.2, 'band2_decay': 0.3, 'band3_decay': 0.4, 'band4_decay': 0.5, 'band5_decay': 0.6, 'band6_decay': 0.7, 'band7_decay': 0.8, 'band8_decay': 0.9, 'band9_decay': 1.0, 'band10_decay': 1.1, 'band11_decay': 1.2, 'mix': 0.7}, 'reason': 'An echo effect for guitar is achieved by emphasizing the higher frequencies with higher gain values, while keeping the lower frequencies minimal to create a sense of space and ambiance. The shorter decay times help maintain the echo effect and prevent the sound from becoming too muddy or washed out.'}

QUESTION: please design a reverb audio effects for a echo drums sound.
ANSWER: {'reverb': {'band0_gain': 0.0, 'band1_gain': 0.0, 'band2_gain': 0.0, 'band3_gain': 0.0, 'band4_gain': 0.0, 'band5_gain': 0.0, 'band6_gain': 0.0, 'band7_gain': 0.0, 'band8_gain': 0.5, 'band9_gain': 1.0, 'band10_gain': 1.0, 'band11_gain': 0.5, 'band0_decay': 0.1, 'band1_decay': 0.1, 'band2_decay': 0.1, 'band3_decay': 0.1, 'band4_decay': 0.1, 'band5_decay': 0.1, 'band6_decay': 0.1, 'band7_decay': 0.1, 'band8_decay': 0.7, 'band9_decay': 0.9, 'band10_decay': 0.9, 'band11_decay': 0.7, 'mix': 0.8}, 'reason': 'Echo drums sound is characterized by a strong focus on high frequencies, along with a short attack and a gradually lengthening decay. This reverb design emphasizes high frequencies by boosting bands 8-11, while keeping the low and mid frequencies flat. The decay times for these high frequency bands are longer than those of the lower bands, creating a sense of an echo. The mix level is set to 0.8 to ensure a balanced level between the dry and wet signals.'}

QUESTION: please design a reverb audio effects for a church guitar sound.
ANSWER:
"""</code></pre>
        </div>
    </section>

    <footer>
        <p>Â© 2025 LLM2Fx Research Team</p>
    </footer>

    <script>
        // Load the master data file
        const masterDataUrl = 'static/master.json';
        let masterData;

        // DOM elements
        const instrumentSelect = document.getElementById('instrument');
        const effectSelect = document.getElementById('effect');
        const variantSelect = document.getElementById('variant');
        const modelsContainer = document.querySelector('.models-container');
        const parameterDisplay = document.getElementById('parameter-display');

        // Fetch the master data
        fetch(masterDataUrl)
            .then(response => response.json())
            .then(data => {
                masterData = data;
                initializeDemo();
            })
            .catch(error => console.error('Error loading master data:', error));
        // Initialize the demo interface
        function initializeDemo() {
            // Set up event listeners
            instrumentSelect.addEventListener('change', updateEffectOptions);
            effectSelect.addEventListener('change', updateVariantOptions);
            variantSelect.addEventListener('change', updateAudioPlayers);

            // Show only the initially selected instrument
            const initialInstrument = instrumentSelect.value;
            document.querySelectorAll('.original-player').forEach(player => {
                player.style.display = player.getAttribute('data-instrument') === initialInstrument ? 'block' : 'none';
            });

            // Initialize the interface
            updateEffectOptions();
        }
        // Update effect options based on selected instrument
        function updateEffectOptions() {
            const selectedInstrument = instrumentSelect.value;
            const instrumentData = masterData.instruments.find(i => i.name === selectedInstrument);

            // Show only the selected instrument's original player
            document.querySelectorAll('.original-player').forEach(player => {
                player.style.display = player.getAttribute('data-instrument') === selectedInstrument ? 'block' : 'none';
            });

            // Update effect options
            if (instrumentData && instrumentData.effects) {
                updateVariantOptions();
            }
        }

        // Update variant options based on selected effect
        function updateVariantOptions() {
            const selectedInstrument = instrumentSelect.value;
            const selectedEffect = effectSelect.value;

            const instrumentData = masterData.instruments.find(i => i.name === selectedInstrument);
            const effectData = instrumentData.effects.find(e => e.fx_type === selectedEffect);

            // Clear existing options
            variantSelect.innerHTML = '';

            // Add new options
            if (effectData && effectData.variants) {
                effectData.variants.forEach(variant => {
                    const option = document.createElement('option');
                    option.value = variant.name;
                    option.textContent = variant.name;
                    variantSelect.appendChild(option);
                });

                // Update audio players
                updateAudioPlayers();
            }
        }

        // Update audio players based on selected options
        function updateAudioPlayers() {
            const selectedInstrument = instrumentSelect.value;
            const selectedEffect = effectSelect.value;
            const selectedVariant = variantSelect.value;

            // Find the relevant data
            const instrumentData = masterData.instruments.find(i => i.name === selectedInstrument);
            const effectData = instrumentData.effects.find(e => e.fx_type === selectedEffect);
            const variantData = effectData.variants.find(v => v.name === selectedVariant);

            // Clear existing players
            modelsContainer.innerHTML = '';

            // Create new players
            if (variantData && variantData.models) {
                variantData.models.forEach((model, index) => {
                    const modelDiv = document.createElement('div');
                    modelDiv.className = 'model-player';
                    if (index === 0) modelDiv.classList.add('selected'); // Auto-select first model

                    const modelName = document.createElement('h3');
                    modelName.textContent = formatModelName(model.name);
                    modelDiv.appendChild(modelName);

                    const audioPlayer = document.createElement('audio');
                    audioPlayer.controls = true;
                    audioPlayer.src = model.audio_path;
                    modelDiv.appendChild(audioPlayer);

                    // Add click handler to display parameters
                    modelDiv.addEventListener('click', () => {
                        // Remove selected class from all models
                        document.querySelectorAll('.model-player').forEach(player => {
                            player.classList.remove('selected');
                        });
                        // Add selected class to clicked model
                        modelDiv.classList.add('selected');
                        displayParameters(model);
                    });

                    modelsContainer.appendChild(modelDiv);
                });

                // Auto-display parameters for the first model
                if (variantData.models.length > 0) {
                    displayParameters(variantData.models[0]);
                }
            } else {
                // Reset parameter display if no models
                parameterDisplay.innerHTML = '<p>No models available</p>';
            }
        }

        // Display parameters for selected model
        function displayParameters(model) {
            // Clear existing content
            parameterDisplay.innerHTML = '';

            // Add model name
            const modelTitle = document.createElement('h4');
            modelTitle.textContent = formatModelName(model.name) + ' Parameters';
            parameterDisplay.appendChild(modelTitle);

            // If there's a JSON path, fetch and display parameters
            if (model.json_path && model.json_path !== '') {
                fetch(model.json_path)
                    .then(response => response.json())
                    .then(params => {
                        // Create parameter display as formatted JSON string
                        const paramContainer = document.createElement('div');
                        paramContainer.className = 'param-container';

                        const jsonString = JSON.stringify(params, null, 2);
                        const preElement = document.createElement('pre');
                        preElement.className = 'param-json';
                        preElement.textContent = jsonString;

                        paramContainer.appendChild(preElement);
                        parameterDisplay.appendChild(paramContainer);
                    })
                    .catch(error => {
                        parameterDisplay.innerHTML += '<p>Error loading parameters</p>';
                        console.error('Error loading parameters:', error);
                    });
            } else if (model.name === 'ground_truth') {
                parameterDisplay.innerHTML += '<p>Ground truth reference audio</p>';
            } else {
                parameterDisplay.innerHTML += '<p>No parameters available</p>';
            }
        }

        // Format model name for display
        function formatModelName(name) {
            if (name === 'ground_truth') return 'Ground Truth';
            if (name === 'llm2fx') return 'LLM2Fx (Ours)';
            if (name === 'gpt4o') return 'GPT-4o';
            if (name === 'chu2025text2fx') return 'Chu2025text2fx';
            return name.charAt(0).toUpperCase() + name.slice(1);
        }

        // Format parameter name for display
        function formatParamName(name) {
            return name
                .replace(/_/g, ' ')
                .replace(/\b\w/g, l => l.toUpperCase());
        }

        // Format parameter value for display
        function formatParamValue(value) {
            if (typeof value === 'number') {
                return value.toFixed(2);
            }
            return value;
        }
    </script>
</body>

</html>
